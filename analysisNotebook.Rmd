---
title: "Unbounded Implicit Learning - Detailed Analysis"
author: "Shanaathanan Modchalingam"
date: "July 2, 2019"
output: 
  html_notebook:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    
---

```{r setup, include=FALSE, warning=FALSE}
rm(list = ls())      # clean environment

source("src/helper_funcs.R")
library(data.table)
library(tidyverse)
library(ggbeeswarm)
library(ez) #for ANOVAs
library(effectsize) # for eta-squared

```

# Some backround

A few studies show that implicit adaptation is 'rigid'. 
'Rigid' in this context means an upper boundary on adaptation.

These studies include (but are not limited to)

### Bond & Taylor (2015)
Here, people adapted to visuomotor rotations and provided aiming directions __prior__ to every reach. 
Implicit learning was measured by subtracting the aiming direction from the direction of movement.  

__Movement type:__ Balistic movements (<400 ms) and end-point feedback  
__Target parameters:__ 7 cm distance, radial target locations    
__Other:__ Participants were reminded "always moving directly to the target may not be effective" every 40 trials  

__Interesting tidbits:__ training to 1 target actually  resulted in higher implicit learning (30°!). 
2 and 4 targets resulted in 17 - 20°.  
Final implicit learning was ~10° for 15, 30, 60 and 90° rotations -- this did not differ between groups.


### Kim et al. (2019)
Here, people adapted to a constant visual error. That is, the error was present and consistent, regardless 

__Movement type:__ Clamped. Slicing movements (<300 ms)  
__Target parameters:__ 8 cm distance, radial target locations, 8 targets (in exp 1) or 4 targets (in exp 2)  
__Other:__ Patricipants were instructed to ignore the visual error.

__Interesting tidbits:__ 


### Modchalingam et al. (2019)

__Movement type:__ Non-balistic  
__Target parameters:__ 12 cm distance, located 45, 90, 135 degrees in polar coordinates  
__Other:__ 

__Interesting tidbits:__ 

# No-cursor analysis

First, we analyze the no-cursor data. Here-in lies our main question; do implicit aftereffects

## Preprocessing

### Load the omnibus data file

First, we will load in the omnibus dataframe. This dataframe contains every no-cursor reach for every participant, including during the aligned training phase. Below is a sample of the dataframe.  

```{r, cols.print = 10, warning=FALSE}

omnibus_nocur <- read_delim("data/omnibus/omnibus_nocur.csv", 
                            delim = ",", 
                            col_types = cols(.default = col_double(), 
                                             task_num = col_double(),
                                             trial_num = col_double(),
                                             rotation_angle = col_double(),
                                             targetangle_deg = col_factor(),
                                             ppt = col_factor(),
                                             stratuse = col_factor(),
                                             exp = col_factor()))

omnibus_nocur <- omnibus_nocur %>% 
  filter(exp != "abruptExp", exp != "reintroExp") %>%
  rename(strat_use = stratuse)
  
# recode exp groups
omnibus_nocur$exp <- recode_factor(omnibus_nocur$exp, "longAbruptExp" = "abrupt",
                                   "gradualExp" = "ramped",
                                   "stepwiseExp" = "stepped")

head(omnibus_nocur)

```

The column of interest is "angular_dev". It contains, in degrees, the amount participants deviated from a straight-line reach towards the target.   

### Baseline correction

The first thing we will do is correct for baseline deviations in training.  
To do this, we will subset the above dataframe into 2 dataframes; one containing data from only the rotated session, and one containing data from only the aligned session.  

```{r}
rot_nocur <- filter(omnibus_nocur, rotation_angle != 0) 

head(rot_nocur)
```

```{r}
bl_nocur <- filter(omnibus_nocur, rotation_angle == 0)

head(bl_nocur)
```

Next, we need to apply a function to the rotated-session data. From each angular deviation, we will subtract the mean angular deviation of all baseline angular deviations that share the same participant and target angle. 


Applying the baseline correction function.

```{r}
rot_nocur$temp <- select(rot_nocur, targetangle_deg, ppt, angular_dev) %>%
  apply(1, FUN = apply_nocur_blcorrection, bl_df = bl_nocur)

# rename some columns
rot_nocur <- 
  rot_nocur %>% 
  rename(raw_angular_dev = angular_dev) %>%
  rename(angular_dev = temp)

head(rot_nocur)
```

We now have an angular deviation column that is baseline corrected. We will use the _rot_nocur_ dataframe from now on.

We need to label the block number for each trial.
```{r}
#first add block number
block_num <- function(df){
  if(df[2] == "ramped"){
    if (df[1] < 20)
      return(1)
    else if (df[1] < 34)
      return(2)
    else if (df[1] < 46)
      return(3)
    else
      return(4)
  }
  else if(df[2] == "reintroExp") {
    if (df[1] < 25)
      return(1)
    else if (df[1] < 38)
      return(2)
    else if (df[1] < 51)
      return(3)
    else
      return(4)
  }
  else {
    if (df[1] < 25)
      return(1)
    else if (df[1] < 37)
      return(2)
    else if (df[1] < 49)
      return(3)
    else
      return(4)
  }
}

rot_nocur$block_num <- apply(rot_nocur[ , c("task_num", "exp")], 1, block_num)
rot_nocur$block_num <- factor(rot_nocur$block_num, levels=c("1", "2", "3", "4"))
  
```


Now we will get a mean for each participant

```{r}
nocur_summary <- rot_nocur %>%
  group_by(exp, ppt, block_num, strat_use) %>%
  summarise(mean_devs = mean(angular_dev), 
            sd = sd(angular_dev), 
            ci = vector_confint(angular_dev),
            n = n(), .groups = "drop")

head(nocur_summary)
```

## Statistics

### OMNIBUS ANOVA
We do a 3x4x2 mixed ANOVA (experiment x block x strategy use)
```{r}
#performing a mixed effects model? (ezANOVA says it can't perform stuff on unbalanced data..)
no_cur_ANOVA <- nocur_summary %>% 
  filter(strat_use == '0') %>%
  ezANOVA(wid = ppt,
        dv = mean_devs, 
        within = .(block_num),
        between = exp,
        detailed = TRUE)

#show and ANOVA + assumptions
print(no_cur_ANOVA)
```

### Implicit aftereffects
We will do 2 sets of analyses for all sets of data. First, we compare our measures in blocks where they first hit 60-degree rotations. Then, we compare block 4 in all groups (time = same).



To test whether the method of perturbation onset effected implicit learning, we compared the aftereffects of learning in No-Strategy trials.

---
1. Initial measures
```{r}
wo_strat_init_blocks <- nocur_summary %>% 
  filter(strat_use == '0') %>% #only the implicit aftereffects
  filter((exp == 'stepped' & block_num == '4') | #only the 5 blocks (1 and 4)
         (exp == 'abrupt' & block_num == '1') |
         (exp == 'ramped' & block_num == '1'))

```

Plot
```{r}
#plot
plot_desc_group_density(wo_strat_init_blocks, "exp", "mean_devs", title = "A. Initial Implicit Aftereffects")
```
```{r}
# descriptives
wo_strat_init_blocks %>% 
  group_by(exp) %>%
  summarise(mean = mean(mean_devs), 
            sd = sd(mean_devs), 
            ci = vector_confint(mean_devs),
            n = n())
```
```{r}
no_cur_ANOVA <- aov(mean_devs ~ exp, data = wo_strat_init_blocks)

#show ANOVA + assumptions
print(summary(no_cur_ANOVA), digits = 6)
eta_squared(no_cur_ANOVA)
TukeyHSD(no_cur_ANOVA)
```
Bayes Factors
```{r}
anovaBF(mean_devs ~ exp, data = wo_strat_init_blocks, progress=FALSE)

bayes_t_test(wo_strat_init_blocks, "exp", "abrupt", "ramped", "mean_devs")
bayes_t_test(wo_strat_init_blocks, "exp", "stepped", "ramped", "mean_devs")
bayes_t_test(wo_strat_init_blocks, "exp", "stepped", "abrupt", "mean_devs")
```





---
2. Time-constant measures

```{r}
wo_strat_final_blocks <- nocur_summary %>% 
  filter(strat_use == '0') %>% #only the implicit aftereffects
  filter(block_num == '4') # only comparing block 4 data
```
```{r}
#plot
plot_desc_group_density(wo_strat_final_blocks, "exp", "mean_devs", title = "B. Block 4 Implicit Aftereffects")
```
```{r}
# descriptives
wo_strat_final_blocks %>% 
  group_by(exp) %>%
  summarise(mean = mean(mean_devs), 
            sd = sd(mean_devs), 
            ci = vector_confint(mean_devs),
            n = n())
```
```{r}
no_cur_ANOVA <- aov(mean_devs ~ exp, data = wo_strat_final_blocks)

#show ANOVA + assumptions
print(summary(no_cur_ANOVA), digits = 6)
eta_squared(no_cur_ANOVA)
TukeyHSD(no_cur_ANOVA)
```
Bayes Factors
```{r}
anovaBF(mean_devs ~ exp, data = wo_strat_final_blocks, progress=FALSE)

bayes_t_test(wo_strat_final_blocks, "exp", "abrupt", "ramped", "mean_devs")
bayes_t_test(wo_strat_final_blocks, "exp", "stepped", "ramped", "mean_devs")
bayes_t_test(wo_strat_final_blocks, "exp", "stepped", "abrupt", "mean_devs")
```




### Explicit strategy
Same analysis pipeline as for the implicit measures

First, we need to actually calculate strategy
```{r}
#first get explicit strat for each ppt

nocur_summary_wide <- nocur_summary %>%
  select(-sd, -ci) %>%
  pivot_wider(names_from = "strat_use", 
              values_from = mean_devs, 
              names_prefix = "strat_") %>%
  mutate(strategy = strat_1 - strat_0)
```

1. Initial measures
```{r}
strat_initial_blocks <- nocur_summary_wide %>% 
  filter((exp == 'stepped' & block_num == '4') | #only the 5 blocks (1 and 4)
         (exp == 'abrupt' & block_num == '1') |
         (exp == 'ramped' & block_num == '1'))
```
```{r}
#plot
plot_desc_group_density(strat_initial_blocks, "exp", "strategy", title = "A. Initial Strategy")
```
```{r}
# descriptives
strat_initial_blocks %>% 
  group_by(exp) %>%
  summarise(mean = mean(strategy), 
            sd = sd(strategy), 
            ci = vector_confint(strategy),
            n = n())
```
```{r}

no_cur_ANOVA <- aov(strategy ~ exp, data = strat_initial_blocks)

#show ANOVA + assumptions
print(summary(no_cur_ANOVA), digits = 6)
eta_squared(no_cur_ANOVA)
TukeyHSD(no_cur_ANOVA)
```

Bayes Factors
```{r}
anovaBF(strategy ~ exp, data = strat_initial_blocks, progress=FALSE)

bayes_t_test(strat_initial_blocks, "exp", "abrupt", "ramped", "strategy")
bayes_t_test(strat_initial_blocks, "exp", "stepped", "ramped", "strategy")
bayes_t_test(strat_initial_blocks, "exp", "stepped", "abrupt", "strategy")
```


---

2. Same but for the final block
```{r}
strat_final_blocks <- nocur_summary_wide %>% 
  filter(block_num == '4')
```


```{r}
#plot
plot_desc_group_density(strat_final_blocks, "exp", "strategy", title = "B. BLock 4 Strategy")
```

```{r}
# descriptives
strat_final_blocks %>% 
  group_by(exp) %>%
  summarise(mean = mean(strategy), 
            sd = sd(strategy), 
            ci = vector_confint(strategy),
            n = n())
```

```{r}

no_cur_ANOVA <- aov(strategy ~ exp, data = strat_final_blocks)

#show ANOVA + assumptions
print(summary(no_cur_ANOVA), digits = 6)
eta_squared(no_cur_ANOVA)
TukeyHSD(no_cur_ANOVA)
```
Bayes Factors
```{r}
anovaBF(strategy ~ exp, data = strat_final_blocks, progress=FALSE)

bayes_t_test(strat_final_blocks, "exp", "abrupt", "ramped", "strategy")
bayes_t_test(strat_final_blocks, "exp", "stepped", "ramped", "strategy")
bayes_t_test(strat_final_blocks, "exp", "stepped", "abrupt", "strategy")
```

3. Compare blocks 1 and 4 for abrupt and gradual data


```{r}
strat_betwn_block <- nocur_summary_wide %>% 
  filter(block_num == '4' | block_num =='1', exp == 'abrupt' | exp == 'ramped')

plot_desc_group_density(strat_betwn_block, "block_num", "strategy", subgroup = "exp", title = "C. Strategy Use blocks 1 v 4")
```

```{r}
anovaBF(strategy ~ exp * block_num, data = strat_betwn_block, progress=FALSE)
```
```{r}
btwn_block_strat_ANOVA <- aov(strategy ~ exp * block_num, data = strat_betwn_block)
print(summary(btwn_block_strat_ANOVA), digits = 6)
eta_squared(btwn_block_strat_ANOVA)
TukeyHSD(btwn_block_strat_ANOVA)
```






























---

# Reach training analysis

## Preprocessing

```{r}
omnibus_training <- read_delim("data/omnibus/omnibus_training.csv", 
                            delim = ",", 
                            col_types = cols(.default = col_double(), 
                                             task_num = col_double(),
                                             trial_num = col_double(),
                                             rotation_angle = col_double(),
                                             targetangle_deg = col_factor(),
                                             stratUse = col_factor(),
                                             ppt = col_factor(),
                                             exp = col_factor()))

omnibus_training <- omnibus_training %>% 
  filter(exp != "abruptExp", exp != 'reintroExp') %>%
  select(-selected, -maxV) %>%
  rename(strat_use = stratUse)

# recode exp groups
omnibus_training$exp <- recode_factor(omnibus_training$exp, "longAbruptExp" = "abrupt",
                                   "gradualExp" = "ramped",
                                   "stepwiseExp" = "stepped")
```

Baseline correction
```{r}
# isolate rotated and bl
rot_training <- filter(omnibus_training, ((exp == "abrupt" | exp == "stepped") & rotation_angle != 0) | (exp == "ramped" & task_num >= 14 ))
bl_training <- filter(omnibus_training, ((exp == "abrupt" | exp == "stepped") & rotation_angle == 0) | (exp == "ramped" & task_num < 14 ))

# apply baseline correction ()
rot_training$temp <- select(rot_training, targetangle_deg, ppt, angular_dev) %>%
  apply(1, FUN = apply_training_blcorrection, bl_df = bl_training)

# rename some columns
rot_training <- 
  rot_training %>% 
  rename(raw_angular_dev = angular_dev) %>%
  rename(angular_dev = temp)

```

We now have bl-corrected rot_training

```{r}
training_summary_per_trial <- rot_training %>%
  group_by(exp, trial_num_cont) %>%
  summarise(mean_devs = mean(angular_dev), sd = sd(angular_dev), 
            ci = vector_confint(angular_dev), .groups = "drop")

#training_summary$rotation_angle <- factor(training_summary$rotation_angle, levels=c("-15", "-30", "-45", "-60"))
training_summary_per_trial$trial_num_cont <- training_summary_per_trial$trial_num_cont - 66

```

We need to label the block number, and trial sets for each trial.
```{r}
# add the block nums (from helperfuncs.R)
rot_training$block_num <- apply(rot_training[ , c("trial_num_cont", "exp")], 
                                1, add_block_num)
rot_training$block_num <- factor(rot_training$block_num, 
                                 levels=c("0", "1", "2", "3", "4", "10"))

# add the trial sets (from helperfuncs.R)
rot_training$trial_set <- apply(rot_training[ , c("trial_num_cont", "exp")], 
                                1, add_trial_set)
rot_training$trial_set <- factor(rot_training$trial_set, 
                                 levels=c("1", "2", "3", "10"))

# fix the trial nums
rot_training$trial_num_cont <- rot_training$trial_num_cont - 66

head(rot_training)
  
```

## Statistics

Plot of relevant groups

```{r}
rot_training_trial_sets <- rot_training %>%
  filter(trial_set != 10) %>%
  group_by(exp, ppt, trial_set) %>%
  summarise(angular_dev_mean = mean(angular_dev), 
            sd = sd(angular_dev), 
            ci = vector_confint(angular_dev),
            n = n(), .groups = "drop")
  # group_by(exp, trial_set) %>%
  # summarise(trial_set_mean = mean(angular_dev_mean), 
  #           sd = sd(angular_dev_mean), 
  #           ci = vector_confint(angular_dev_mean),
  #           n = n(), .groups = "drop")

head(rot_training_trial_sets)
```

```{r}
plot_desc_group_density(rot_training_trial_sets, "trial_set", "angular_dev_mean", subgroup = "exp", title = "Training trial sets")
```

We confirm that all groups learn

```{r}
training_final_trial_set <- rot_training_trial_sets %>% 
  filter(trial_set == '3')
```

```{r}
#plot
plot_desc_group_density(training_final_trial_set, "exp", "angular_dev_mean", title = "Block 4 Adaptation")
```

```{r}
# descriptives
training_final_trial_set %>% 
  group_by(exp) %>%
  summarise(mean = mean(angular_dev_mean), 
            norm_mean = mean(angular_dev_mean)/60,
            sd = sd(angular_dev_mean), 
            ci = vector_confint(angular_dev_mean),
            n = n())
```

```{r}

training_ANOVA <- aov(angular_dev_mean ~ exp, data = training_final_trial_set)

#show ANOVA + assumptions
print(summary(training_ANOVA), digits = 6)
eta_squared(training_ANOVA)
TukeyHSD(training_ANOVA)
```

Bayes Factors
```{r}
anovaBF(angular_dev_mean ~ exp, data = training_final_trial_set, progress=FALSE)

bayes_t_test(training_final_trial_set, "exp", "abrupt", "ramped", "angular_dev_mean")
bayes_t_test(training_final_trial_set, "exp", "stepped", "ramped", "angular_dev_mean")
bayes_t_test(training_final_trial_set, "exp", "stepped", "abrupt", "angular_dev_mean")
```

----
# Combining all data

We need to combine both omnibus dataframes
```{r}
# stitch the two together
rot_training_t <- rot_training %>% 
  select(-trial_set, -trial_num_cont) %>%
  arrange(exp)

rot_nocur_t <- rot_nocur %>%
  arrange(exp, ppt, task_num)

# making continuous trial numbers
trials_abr_step_train <- c(1:45, 64:84, 103:147, 166:186, 205:249, 268:288, 307:351, 370:390) #these are all the training trials
trials_ramp_train <- c(1:66, 85:129, 148:168, 187:231, 250:270, 289:333, 352:372) # the numbers are different for the ramped experiment (missing a block of nocur)

# repeat these based on the number of ppt and concat
reps_abr_train <- rep(trials_abr_step_train, length(unique(filter(rot_training_t, exp == 'abrupt')$ppt)))
reps_step_train <- rep(trials_abr_step_train, length(unique(filter(rot_training_t, exp == 'stepped')$ppt)))
reps_ramp_train <- rep(trials_ramp_train, length(unique(filter(rot_training_t, exp == 'ramped')$ppt))) + 18
reps_train <- c(reps_abr_train, reps_ramp_train, reps_step_train)

# add the column to rot_training_t
rot_training_t$trial_num_cont <- reps_train


# making continuous trial numbers for nocursors
# get the missing trials (nocusors)
temp <- 1:408
reps_abr_nc <- rep(temp[!(temp %in% trials_abr_step_train)], length(unique(filter(rot_training_t, exp == 'abrupt')$ppt)))
reps_step_nc <- rep(temp[!(temp %in% trials_abr_step_train)], length(unique(filter(rot_training_t, exp == 'stepped')$ppt)))

temp <- 1:390
reps_ramp_nc <- rep( temp[!(temp %in% trials_ramp_train)], length(unique(filter(rot_training_t, exp == 'ramped')$ppt))) + 18
reps_nc <- c(reps_abr_nc, reps_ramp_nc, reps_step_nc)

# add to rot_nocur_t
rot_nocur_t$trial_num_cont <- reps_nc
  
  
rot_all <- bind_rows(rot_training_t, rot_nocur_t) %>%
  arrange(exp, ppt, trial_num_cont) %>%
  mutate(norm_angular_dev = angular_dev / rotation_angle * -1)

# make a summary df
rot_all_summary <- rot_all %>%
  group_by(exp, trial_num_cont, strat_use) %>%
  summarise(mean = mean(angular_dev),
            sd = sd(angular_dev), 
            ci = vector_confint(angular_dev),
            n = n(), .groups = "drop")
rot_all_summary$strat_use <- as.factor(rot_all_summary$strat_use)


```


plot
```{r}
p <- rot_all %>% 
  ggplot(aes(trial_num_cont, angular_dev, colour = exp, shape = strat_use)) +
  geom_point(alpha = 0.04)+
  geom_point(data = rot_all_summary, aes(y = mean)) +
  scale_shape_manual(values=c(20, 2))+
  # stat_summary(fun=mean, geom="point", size=3, color="red") +
  theme_minimal() +
  theme(panel.grid.major.y = element_line(colour = "#CCCCCC")) +
  scale_y_continuous(limits = c(-20, 90)) +
  ggtitle("All data")

p

# ggsave(p, height = 9, width = 16, device = "svg", filename = "data/paper_figs/all_reaches.svg")
```

Repeat for normalized angular deviations
```{r}
rot_norm_summary <- rot_all %>%
  group_by(exp, trial_num_cont, strat_use) %>%
  summarise(mean = mean(norm_angular_dev),
            sd = sd(norm_angular_dev), 
            ci = vector_confint(norm_angular_dev),
            n = n(), .groups = "drop")
rot_norm_summary$strat_use <- as.factor(rot_norm_summary$strat_use)
```
plot
```{r}
p <- rot_all %>% 
  ggplot(aes(trial_num_cont, norm_angular_dev, colour = exp, shape = strat_use)) +
  geom_point(alpha = 0.04) +
  geom_point(data = rot_norm_summary, aes(y = mean)) +
  scale_shape_manual(values=c(20, 2))+
  # stat_summary(fun=mean, geom="point", size=3, color="red") +
  theme_minimal() +
  theme(panel.grid.major.y = element_line(colour = "#CCCCCC")) +
  scale_y_continuous(limits = c(-0.5, 1.5)) +
  ggtitle("All data")

p

# ggsave(p, height = 9, width = 16, device = "svg", filename = "data/paper_figs/all_reaches_norm.svg")
```


# Additional analysis
Bayesian analysis if block 4 implicit aftereffects using the rethinking package
```{r}
library(rethinking)
head(wo_strat_final_blocks)
```
Here is how our model would look:
$$
\begin{aligned}
I_i \;&\sim\; Normal(\mu_i, \sigma) \\
\mu_i \;&=\; \alpha_{exp[i]} \\
\alpha_j \;&\sim\; Normal(15, 5) \;\;\;\; for \; j = 1,2,3 \\
\sigma \;&\sim\; Exponential(1)
\end{aligned}

$$

```{r}
wo_strat_final_blocks$exp_id <- as.integer(wo_strat_final_blocks$exp)

m_imp_b4 <-quap(
  alist(
    mean_devs ~ dnorm(mu,sigma),
    mu <- a[exp_id],
    a[exp_id] ~ dnorm(15,5),
    sigma ~ dexp(1)
    ), data = wo_strat_final_blocks)

labels <-paste("a[",1:3,"]:",levels(wo_strat_final_blocks$exp),sep="")
plot(precis(m_imp_b4, depth=2, pars="a"), labels=labels)
```
```{r}
precis( extract.samples(m_imp_b4),depth=2)
```



----
# Older reach training analysis
We use the blocks of interest
```{r}
rot_training_ana_blocks <- rot_training %>%
  filter(block_num != 10) %>%
  group_by(exp, ppt, block_num) %>%
  summarise(angular_dev_mean = mean(angular_dev), 
            sd = sd(angular_dev), 
            ci = vector_confint(angular_dev),
            n = n(), .groups = "drop")
```

### Training ANOVA
This one will need to be a 3xN ANOVA (exp x blocks), where N is the number of blocks being used.
```{r}
training_anova <- ezANOVA( data = rot_training_ana_blocks,
                           wid = ppt,
                           dv = angular_dev_mean, 
                           within = block_num,
                           between = exp,
                           detailed = FALSE,
                           return_aov = TRUE)
print(training_anova)
```
### Post hoc test: 
Note the Tukey's HSD is for INDEPENDENT tests. Doesn't work for repeated measures like we have here (blocks)
For the old paper, we just report ANOVA results, no post hocs for these.

```{r}
#descriptives
desc_temp <- rot_training_ana_blocks %>%
  group_by(exp, block_num) %>%
  summarise(mean_dev = mean(angular_dev_mean), 
            sd = sd(angular_dev_mean), 
            ci = vector_confint(angular_dev_mean),
            n = n(), .groups = "drop")

desc_temp
```
### Plot
```{r}
p <- rot_training_ana_blocks %>%
  ggplot(aes(block_num, angular_dev_mean, colour = exp)) +
  ggtitle("Training over blocks and experiments") +
  geom_beeswarm(dodge.width = 0.6, alpha = 0.2) +
  geom_point(data = desc_temp,
             aes(block_num, mean_dev, colour = exp),
             size = 5, alpha = 0.6, 
             position = position_dodge(width = .6)) + 
  geom_linerange(data = desc_temp,
                 aes(block_num, mean_dev, colour = exp, 
                     ymin = mean_dev - ci, ymax = mean_dev + ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6)) +
  scale_y_continuous(limits = c(-10, 75), 
                     breaks = c(0, 15, 30, 45, 60), 
                     name = "hand deviation (°)") +
  scale_x_discrete(name = "block", 
                   labels = c("Initial", '1', '2', '3', '4')) +
  theme_minimal() +
  theme(panel.grid.major.y = element_line(colour = "#CCCCCC")) +
  scale_colour_manual(values=c( "#d40000", "#084594", "#8365b5"), 
                       breaks=c("stepped", "abrupt", "ramped"),
                       labels=c( "stepwise", "abrupt", "gradual")) +
  NULL
p

p <- p +
    theme(text = element_text(size=20), 
        axis.text = element_text(size=20), 
        legend.text = element_text(size=24))

# ggsave(p, height = 7, width = 10, device = "svg", filename = "data/paper_figs/training_plot_blocks.svg")
```
# Additional Analyses
## Rebounds

We start with the baseline-corrected reach data (rot_training).
```{r}
# make a list of rebound trials
rebound_trials <- c(46, 67, 112, 133, 178, 199, 244) #block 4 only has 1 rebound trial
pre_reb_trials <- rebound_trials - 1

rot_rebounds <- rot_training %>%
  filter(trial_num_cont %in% rebound_trials) %>%
  select(-block_num)

rot_rebounds$pre_reb_dev <- rot_training %>%
  filter(trial_num_cont %in% pre_reb_trials) %>%
  select(angular_dev) %>% as_vector()

rot_rebounds$decay <- rot_rebounds$pre_reb_dev - rot_rebounds$angular_dev

rot_rebounds$trial_num_cont <- as.factor(rot_rebounds$trial_num_cont)

```



```{r}

#descriptives
desc_temp <- rot_rebounds %>%
  group_by(exp, trial_num_cont, NULL) %>%
  summarise(mean_dev = mean(angular_dev), 
            sd = sd(angular_dev), 
            ci = vector_confint(angular_dev),
            n = n(), .groups = "drop")
desc_temp$trial_num_cont <- as.factor(desc_temp$trial_num_cont)
rot_rebounds$trial_num_cont <- as.factor(rot_rebounds$trial_num_cont)

p <- desc_temp %>%
  ggplot(aes(trial_num_cont, mean_dev, colour = exp)) +
  ggtitle("Rebounds during training") +
  geom_beeswarm(data = rot_rebounds,
                aes(y = angular_dev),
                dodge.width = 0.6, alpha = 0.2) +
  geom_point(size = 5, alpha = 0.6, 
             position = position_dodge(width = .6)) + 
  geom_linerange(aes(ymin = mean_dev - ci, ymax = mean_dev + ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6))

p

```


```{r}
#descriptives
desc_temp <- rot_rebounds %>%
  group_by(exp, trial_num_cont) %>%
  summarise(mean_decay = mean(decay), 
            sd = sd(decay), 
            ci = vector_confint(decay),
            n = n(), .groups = "drop")

p <- desc_temp %>%
  ggplot(aes(trial_num_cont, mean_decay, colour = exp)) +
  ggtitle("Decay during no-cursor trials training") +
  geom_beeswarm(data = rot_rebounds,
                aes(y = decay),
                dodge.width = 0.6, alpha = 0.2) +
  geom_point(size = 5, alpha = 0.6, 
             position = position_dodge(width = .6)) + 
  geom_linerange(aes(ymin = mean_decay - ci, ymax = mean_decay + ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6)) +
  scale_y_continuous(limits = c(-30, 75), 
                     breaks = c(0, 15, 30, 45, 60), 
                     name = "decay over no-cursor trials (°)")

p

```


## Measures of explicit learning

We measure explicit adaptation, but it is sometimes taken as the difference between performance in training trials and implicit probes.
Note: It may be that contextual cues like the cursor being present matters (e.g. in contextual inference models)

Let's compare the 2:
per ppt, we need.. implicit learning per block (from nucursor data), training performance per block (from the previous section)

```{r}
# we can start with the implicit data. Formatted so that each participant has some values for each block
pre_reb_devs <- rot_rebounds %>% 
  filter(trial_num_cont %in% c(46, 112, 178, 244)) %>%
  select(ppt, pre_reb_dev) 

pre_reb_devs <- pre_reb_devs %>% 
  bind_cols(block_num = as.factor(rep(1:4, nrow(pre_reb_devs)/4)))
  
per_ppt_format <- nocur_summary %>%
  select(-sd, -ci, -n) %>% #getting rid of sanity check variables
  spread(strat_use, mean_devs) %>%
  rename(without_strat_dev = '0'  , with_strat_dev = '1')

combined_data <- left_join(per_ppt_format, pre_reb_devs, by = c("ppt", "block_num")) %>%
  mutate(measured_exp = with_strat_dev - without_strat_dev, inferred_exp = pre_reb_dev - without_strat_dev)


head(combined_data)
```

Plot the different explicit measures

```{r}
#descriptives
desc_temp <- combined_data %>%
  group_by(exp, block_num) %>%
  summarise(measured_exp_mean = mean(measured_exp), 
            measured_exp_sd = sd(measured_exp), 
            measured_exp_ci = vector_confint(measured_exp),
            inferred_exp_mean = mean(inferred_exp), 
            inferred_exp_sd = sd(inferred_exp), 
            inferred_exp_ci = vector_confint(inferred_exp),
            n = n(), .groups = "drop")
head(desc_temp)
```

```{r}
p <- desc_temp %>%
  ggplot(aes(block_num, measured_exp_mean, colour = exp)) +
  ggtitle("Measured vs inferred explicit (circles = measured)") +
  geom_point(size = 5, alpha = 0.6, 
             position = position_dodge(width = .6)) + 
  geom_linerange(aes(ymin = measured_exp_mean - measured_exp_ci, ymax = measured_exp_mean + measured_exp_ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6)) +
  geom_point(aes(y = inferred_exp_mean),
             size = 5, alpha = 0.6, shape = 18,
             position = position_dodge(width = .6)) + 
  geom_linerange(aes(ymin = inferred_exp_mean - inferred_exp_ci, ymax = inferred_exp_mean + inferred_exp_ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6)) + 
  scale_y_continuous(name = "explicit learning (°)") +
  scale_x_discrete(name = "block")
  
p
```
Measured explicit learning = markedly lower

# Plots for print
```{r}
plot_nice_group_density(training_final_trial_set, exp, angular_dev_mean, title = "Block 4 Adaptation")
```


