---
title: "Unbounded Implicit Learning - Detailed Analysis"
author: "Shanaathanan Modchalingam"
date: "July 2, 2019"
output: 
  html_notebook:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    
---

```{r setup, include=FALSE, warning=FALSE}
source("analysisFunctions.R")
source("helper_funcs.R")
library(data.table)
library(tidyverse)
library(ggbeeswarm)
library(ez) #for ANOVAs
library(effectsize) # for eta-squared
```

# Some backround

A few studies show that implicit adaptation is 'rigid'. 
'Rigid' in this context means an upper boundary on adaptation.

These studies include (but are not limited to)

### Bond & Taylor (2015)
Here, people adapted to visuomotor rotations and provided aiming directions __prior__ to every reach. 
Implicit learning was measured by subtracting the aiming direction from the direction of movement.  

__Movement type:__ Balistic movements (<400 ms) and end-point feedback  
__Target parameters:__ 7 cm distance, radial target locations    
__Other:__ Participants were reminded "always moving directly to the target may not be effective" every 40 trials  

__Interesting tidbits:__ training to 1 target actually  resulted in higher implicit learning (30°!). 
2 and 4 targets resulted in 17 - 20°.  
Final implicit learning was ~10° for 15, 30, 60 and 90° rotations -- this did not differ between groups.


### Kim et al. (2019)
Here, people adapted to a constant visual error. That is, the error was present and consistent, regardless 

__Movement type:__ Clamped. Slicing movements (<300 ms)  
__Target parameters:__ 8 cm distance, radial target locations, 8 targets (in exp 1) or 4 targets (in exp 2)  
__Other:__ Patricipants were instructed to ignore the visual error.

__Interesting tidbits:__ 


### Modchalingam et al. (20??)

__Movement type:__ Non-balistic  
__Target parameters:__ 12 cm distance, located 45, 90, 135 degrees in polar coordinates  
__Other:__ 

__Interesting tidbits:__ 

# No-cursor analysis

First, we analyze the no-cursor data. Here-in lies our main question; do implicit aftereffects

## Preprocessing

### Load the omnibus data file

First, we will load in the omnibus dataframe. This dataframe contains every no-cursor reach for every participant, including during the aligned training phase. Below is a sample of the dataframe.  

```{r, cols.print = 10, warning=FALSE}

omnibus_nocur <- read_delim("data/omnibus/omnibus_nocur.csv", 
                            delim = ",", 
                            col_types = cols(.default = col_double(), 
                                             task_num = col_double(),
                                             trial_num = col_double(),
                                             rotation_angle = col_factor(),
                                             targetangle_deg = col_factor(),
                                             ppt = col_factor(),
                                             stratuse = col_factor(),
                                             exp = col_factor()))

omnibus_nocur <- omnibus_nocur %>% 
  filter(exp != "abruptExp", exp != "reintroExp")

head(omnibus_nocur)

```

The column of interest is "angular_dev". It contains, in degrees, the amount participants deviated from a straight-line reach towards the target.   

### Baseline correction

The first thing we will do is correct for baseline deviations in training.  
To do this, we will subset the above dataframe into 2 dataframes; one containing data from only the rotated session, and one containing data from only the aligned session.  

```{r}
rot_nocur <- filter(omnibus_nocur, rotation_angle != 0) 

head(rot_nocur)
```

```{r}
bl_nocur <- filter(omnibus_nocur, rotation_angle == 0)

head(bl_nocur)
```

Next, we need to apply a function to the rotated-session data. From each angular deviation, we will subtract the mean angular deviation of all baseline angular deviations that share the same participant and target angle. 

The functions is as follows. 

```{r}
apply_blcorrection <- function(rot_df_row, bl_df){
  
  # make sure input is in format: targetangle_deg, ppt, angular_dev

  bl <- filter(bl_df, 
               targetangle_deg == rot_df_row[1] & 
                 ppt == rot_df_row[2])$angular_dev %>% 
    mean(na.rm = TRUE)

  
  corrected_dev <- as.numeric(rot_df_row[3]) - bl
  
  return(corrected_dev)
}
```

Applying the above function to rotated-session-data.

```{r}
rot_nocur$temp <- select(rot_nocur, targetangle_deg, ppt, angular_dev) %>%
  apply(1, FUN = apply_blcorrection, bl_df = bl_nocur)

# rename some columns
rot_nocur <- 
  rot_nocur %>% 
  rename(raw_angular_dev = angular_dev) %>%
  rename(angular_dev = temp)

head(rot_nocur)
```

We now have an angular deviation column that is baseline corrected. We will use the _rot_nocur_ dataframe from now on.

We need to label the block number for each trial.
```{r}
#first add block number
block_num <- function(df){
  if(df[2] == "gradualExp"){
    if (df[1] < 20)
      return(1)
    else if (df[1] < 34)
      return(2)
    else if (df[1] < 46)
      return(3)
    else
      return(4)
  }
  else if(df[2] == "reintroExp") {
    if (df[1] < 25)
      return(1)
    else if (df[1] < 38)
      return(2)
    else if (df[1] < 51)
      return(3)
    else
      return(4)
  }
  else {
    if (df[1] < 25)
      return(1)
    else if (df[1] < 37)
      return(2)
    else if (df[1] < 49)
      return(3)
    else
      return(4)
  }
}

rot_nocur$block_num <- apply(rot_nocur[ , c("task_num", "exp")], 1, block_num)
rot_nocur$block_num <- factor(rot_nocur$block_num, levels=c("1", "2", "3", "4"))
rot_nocur$rotation_angle <- factor(rot_nocur$rotation_angle, levels=c("-15", "-30", "-45", "-60"))
  
```


__We might want to use group by to get participant means!__

Now we will get a mean for each participant

```{r}
nocur_summary <- rot_nocur %>%
  group_by(exp, ppt, rotation_angle, block_num, stratuse) %>%
  summarise(mean_devs = mean(angular_dev), 
            sd = sd(angular_dev), 
            ci = vector_confint(angular_dev),
            n = n())

head(nocur_summary)
```

## Statistics

### OMNIBUS ANOVA
We do a 3x4x2 mixed ANOVA (experiment x block x strategy use)
```{r}
#performing a mixed effects model? (ezANOVA says it can't perform stuff on unbalanced data..)
no_cur_ANOVA <- nocur_summary %>% 
  filter(stratuse == '0') %>%
  ezANOVA(wid = ppt,
        dv = mean_devs, 
        within = .(block_num),
        between = exp,
        detailed = TRUE)

#show and ANOVA + assumptions
print(no_cur_ANOVA)
```

### Implicit aftereffects
We will do 2 sets of analyses for all sets of data. First, we compare our measures in blocks where they first hit 60-degree rotations. Then, we compare block 4 in all groups (time = same).



To test whether the method of perturbation onset effected implicit learning, we compared the aftereffects of learning in No-Strategy trials.

---
1. Initial measures
```{r}
wo_strat_init_blocks <- nocur_summary %>% 
  filter(stratuse == '0') %>% #only the implicit aftereffects
  filter((exp == 'stepwiseExp' & block_num == '4') | #only the 5 blocks (1 and 4)
         (exp == 'longAbruptExp' & block_num == '1') |
         (exp == 'gradualExp' & block_num == '1'))


no_cur_ANOVA <- aov(mean_devs ~ exp, data = wo_strat_init_blocks)

#show and ANOVA + assumptions
print(summary(no_cur_ANOVA), digits = 6)
eta_squared(no_cur_ANOVA)
TukeyHSD(no_cur_ANOVA)
```
Plot
```{r}
#plot
plot_group_density(wo_strat_init_blocks, "exp", "mean_devs", title = "Initial Implicit Aftereffects")
```
```{r}
# descriptives
wo_strat_init_blocks %>% 
  group_by(exp) %>%
  summarize(mean = mean(mean_devs), 
            sd = sd(mean_devs), 
            ci = vector_confint(mean_devs),
            n = n())
```


---
2. Time-constant measures

```{r}
wo_strat_final_blocks <- nocur_summary %>% 
  filter(stratuse == '0') %>% #only the implicit aftereffects
  filter(block_num == '4') # only comparing block 4 data


no_cur_ANOVA <- aov(mean_devs ~ exp, data = wo_strat_final_blocks)

#show and ANOVA + assumptions
print(summary(no_cur_ANOVA), digits = 6)
eta_squared(no_cur_ANOVA)
TukeyHSD(no_cur_ANOVA)
```

```{r}
#plot
plot_group_density(wo_strat_final_blocks, "exp", "mean_devs", title = "Block 4 Implicit Aftereffects")
```

```{r}
# descriptives
wo_strat_final_blocks %>% 
  group_by(exp) %>%
  summarize(mean = mean(mean_devs), 
            sd = sd(mean_devs), 
            ci = vector_confint(mean_devs),
            n = n())
```




### Explicit strategy
Same analysis pipeline as for the implicit measures

First, we need to actually calculate strategy
```{r}
#first get explicit strat for each ppt

nocur_summary_wide <- nocur_summary %>%
  select(-sd, -ci) %>%
  pivot_wider(names_from = "stratuse", 
              values_from = mean_devs, 
              names_prefix = "strat_") %>%
  mutate(strategy = strat_1 - strat_0)
```

1. Initial measures
```{r}
strat_initial_blocks <- nocur_summary_wide %>% 
  filter((exp == 'stepwiseExp' & block_num == '4') | #only the 5 blocks (1 and 4)
         (exp == 'longAbruptExp' & block_num == '1') |
         (exp == 'gradualExp' & block_num == '1'))


no_cur_ANOVA <- aov(strategy ~ exp, data = strat_initial_blocks)

#show and ANOVA + assumptions
print(summary(no_cur_ANOVA), digits = 6)
eta_squared(no_cur_ANOVA)
TukeyHSD(no_cur_ANOVA)
```

```{r}
#plot
plot_group_density(strat_initial_blocks, "exp", "strategy", title = "Initial Strategy")
```

```{r}
# descriptives
strat_initial_blocks %>% 
  group_by(exp) %>%
  summarize(mean = mean(strategy), 
            sd = sd(strategy), 
            ci = vector_confint(strategy),
            n = n())
```

---

2. Same but for the final block
```{r}
strat_final_blocks <- nocur_summary_wide %>% 
  filter(block_num == '4')

no_cur_ANOVA <- aov(strategy ~ exp, data = strat_final_blocks)

#show and ANOVA + assumptions
print(summary(no_cur_ANOVA), digits = 6)
eta_squared(no_cur_ANOVA)
TukeyHSD(no_cur_ANOVA)
```


```{r}
#plot
plot_group_density(strat_final_blocks, "exp", "strategy", title = "BLock 4 Strategy")
```


```{r}
# descriptives
strat_final_blocks %>% 
  group_by(exp) %>%
  summarize(mean = mean(strategy), 
            sd = sd(strategy), 
            ci = vector_confint(strategy),
            n = n())
```


---

# Reach training analysis

## Preprocessing

```{r}
omnibus_training <- read_delim("data/omnibus/omnibus_training.csv", 
                            delim = ",", 
                            col_types = cols(.default = col_double(), 
                                             task_num = col_double(),
                                             trial_num = col_double(),
                                             rotation_angle = col_factor(),
                                             targetangle_deg = col_factor(),
                                             ppt = col_factor(),
                                             exp = col_factor()))

omnibus_training <- omnibus_training %>% 
  filter(exp != "abruptExp", exp != 'reintroExp') %>%
  select(-trial_num)


# isolate rotated and bl
rot_training <- filter(omnibus_training, rotation_angle != 0)
bl_training <- filter(omnibus_training, rotation_angle == 0)


apply_blcorrection <- function(rot_df_row, bl_df){
  
  # make sure input is in format: targetangle_deg, ppt, angular_dev

  bl <- filter(bl_df, 
               targetangle_deg == rot_df_row[1] & 
                 ppt == rot_df_row[2])$angular_dev %>% 
    mean(na.rm = TRUE)

  
  corrected_dev <- as.numeric(rot_df_row[3]) - bl
  
  return(corrected_dev)
}


rot_training$temp <- select(rot_training, targetangle_deg, ppt, angular_dev) %>%
  apply(1, FUN = apply_blcorrection, bl_df = bl_training)

# rename some columns
rot_training <- 
  rot_training %>% 
  rename(raw_angular_dev = angular_dev) %>%
  rename(angular_dev = temp)

```

We now have bl-corrected rot_training

```{r}
training_summary_per_trial <- rot_training %>%
  group_by(exp, trial_num_cont) %>%
  summarise(mean_devs = mean(angular_dev), sd = sd(angular_dev), 
            ci = vector_confint(angular_dev))

#training_summary$rotation_angle <- factor(training_summary$rotation_angle, levels=c("-15", "-30", "-45", "-60"))
training_summary_per_trial$trial_num_cont <- training_summary_per_trial$trial_num_cont - 66

```

We need to label the block number for each trial.
```{r}
#first add block number
block_num <- function(df){
  # these if statements are repetitive at the moment. I've kept them in case different trials need to be isolated for different experiment protocols
  x <- as.double(df[1])
  if(df[2] == "gradualExp"){
    if (x <= 69)
      return(0)
    else if (x >= 127 & x <= 132)
      return(1)
    else if (x >= 193 & x <= 198)
      return(2)
    else if (x >= 259 & x <= 264)
      return(3)
    else if (x >= 325 & x <= 330)
      return(4)
    else
      return(10)
  }
  else if(df[2] == "stepwiseExp") {
    if (x <= 69)
      return(0)
    else if (x >= 127 & x <= 132)
      return(1)
    else if (x >= 193 & x <= 198)
      return(2)
    else if (x >= 259 & x <= 264)
      return(3)
    else if (x >= 325 & x <= 330)
      return(4)
    else
      return(10)
  }
  else { #for longAbrupt
    if (x <= 69)
      return(0)
    else if (x >= 127 & x <= 132)
      return(1)
    else if (x >= 193 & x <= 198)
      return(2)
    else if (x >= 259 & x <= 264)
      return(3)
    else if (x >= 325 & x <= 330)
      return(4)
    else
      return(10)
  }
}

rot_training$block_num <- apply(rot_training[ , c("trial_num_cont", "exp")], 
                                1, block_num)
rot_training$block_num <- factor(rot_training$block_num, 
                                 levels=c("0", "1", "2", "3", "4", "10"))
head(rot_training)
  
```

## Statistics

First, we will just use the blocks of interest
```{r}
rot_training_ana_blocks <- rot_training %>%
  filter(block_num != 10) %>%
  group_by(exp, ppt, block_num) %>%
  summarize(angular_dev_mean = mean(angular_dev), 
            sd = sd(angular_dev), 
            ci = vector_confint(angular_dev),
            n = n())
```

### Training ANOVA
This one will need to be a 3xN ANOVA (exp x blocks), where N is the number of blocks being used.
```{r}
training_anova <- ezANOVA( data = rot_training_ana_blocks,
                           wid = ppt,
                           dv = angular_dev_mean, 
                           within = block_num,
                           between = exp,
                           detailed = FALSE,
                           return_aov = TRUE)
print(training_anova)
```
### Post hoc test: 
Note the Tukey's HSD is for INDEPENDENT tests. Doesn't work for repeated measures like we have here (blocks)
For the old paper, we just report ANOVA results, no post hocs for these.

```{r}
#descriptives
desc_temp <- rot_training_ana_blocks %>%
  group_by(exp, block_num) %>%
  summarize(mean_dev = mean(angular_dev_mean), 
            sd = sd(angular_dev_mean), 
            ci = vector_confint(angular_dev_mean),
            n = n())

desc_temp
```
### Plot
```{r}
p <- rot_training_ana_blocks %>%
  ggplot(aes(block_num, angular_dev_mean, colour = exp)) +
  ggtitle("Training over blocks and experiments") +
  geom_beeswarm(dodge.width = 0.6, alpha = 0.2) +
  geom_point(data = desc_temp,
             aes(block_num, mean_dev, colour = exp),
             size = 5, alpha = 0.6, 
             position = position_dodge(width = .6)) + 
  geom_linerange(data = desc_temp,
                 aes(block_num, mean_dev, colour = exp, 
                     ymin = mean_dev - ci, ymax = mean_dev + ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6)) +
  scale_y_continuous(limits = c(-10, 75), 
                     breaks = c(0, 15, 30, 45, 60), 
                     name = "hand deviation (°)") +
  scale_x_discrete(name = "block", 
                   labels = c("Initial", '1', '2', '3', '4')) +
  theme_minimal() +
  theme(panel.grid.major.y = element_line(colour = "#CCCCCC")) +
  scale_colour_manual(values=c( "#d40000", "#084594", "#8365b5"), 
                       breaks=c("stepwiseExp", "longAbruptExp", "gradualExp"),
                       labels=c( "stepwise", "abrupt", "gradual")) +
  NULL
p

p <- p +
    theme(text = element_text(size=20), 
        axis.text = element_text(size=20), 
        legend.text = element_text(size=24))

# ggsave(p, height = 7, width = 10, device = "svg", filename = "data/paper_figs/training_plot_blocks.svg")
```
# Additional Analyses
## Rebounds

We will start with the baseline-corrected reach data (rot_training).
```{r}
# make a list of rebound trials
rebound_trials <- c(112, 133, 178, 199, 244, 265, 310) #block 4 only has 1 rebound trial
pre_reb_trials <- rebound_trials - 1

rot_rebounds <- rot_training %>%
  filter(trial_num_cont %in% rebound_trials) %>%
  select(-block_num)

rot_rebounds$pre_reb_dev <- rot_training %>%
  filter(trial_num_cont %in% pre_reb_trials) %>%
  select(angular_dev) %>% as_vector()

rot_rebounds$decay <- rot_rebounds$pre_reb_dev - rot_rebounds$angular_dev

rot_rebounds$trial_num_cont <- as.factor(rot_rebounds$trial_num_cont)

```



```{r}

#descriptives
desc_temp <- rot_rebounds %>%
  group_by(exp, trial_num_cont) %>%
  summarize(mean_dev = mean(angular_dev), 
            sd = sd(angular_dev), 
            ci = vector_confint(angular_dev),
            n = n())
desc_temp$trial_num_cont <- as.factor(desc_temp$trial_num_cont)
rot_rebounds$trial_num_cont <- as.factor(rot_rebounds$trial_num_cont)

p <- desc_temp %>%
  ggplot(aes(trial_num_cont, mean_dev, colour = exp)) +
  ggtitle("Rebounds during training") +
  geom_beeswarm(data = rot_rebounds,
                aes(y = angular_dev),
                dodge.width = 0.6, alpha = 0.2) +
  geom_point(size = 5, alpha = 0.6, 
             position = position_dodge(width = .6)) + 
  geom_linerange(aes(ymin = mean_dev - ci, ymax = mean_dev + ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6))

p

```
```{r}
#descriptives
desc_temp <- rot_rebounds %>%
  group_by(exp, trial_num_cont) %>%
  summarize(mean_decay = mean(decay), 
            sd = sd(decay), 
            ci = vector_confint(decay),
            n = n())

p <- desc_temp %>%
  ggplot(aes(trial_num_cont, mean_decay, colour = exp)) +
  ggtitle("Decay during no-cursor trials training") +
  geom_beeswarm(data = rot_rebounds,
                aes(y = decay),
                dodge.width = 0.6, alpha = 0.2) +
  geom_point(size = 5, alpha = 0.6, 
             position = position_dodge(width = .6)) + 
  geom_linerange(aes(ymin = mean_decay - ci, ymax = mean_decay + ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6)) +
  scale_y_continuous(limits = c(-30, 75), 
                     breaks = c(0, 15, 30, 45, 60), 
                     name = "decay over no-cursor trials (°)")

p

```
## Measures of explicit learning

We measure explicit adaptation, but it is sometimes taken as the difference between performance in training trials and implicit probes.
Note: It may be that contextual cues like the cursor being present matters (e.g. in contextual inference models)

Let's compare the 2:
per ppt, we need.. implicit learning per block (from nucursor data), training performance per block (from the previous section)

```{r}
# we can start with the implicit data. Formatted so that each participant has some values for each block
pre_reb_devs <- rot_rebounds %>% 
  filter(trial_num_cont %in% c(112, 178, 244, 310)) %>%
  select(ppt, pre_reb_dev) %>%
  bind_cols(block_num = as.factor(rep(1:4, 106)))
  
per_ppt_format <- nocur_summary %>%
  select(-sd, -ci, -n) %>% #getting rid of sanity check variables
  spread(stratuse, mean_devs) %>%
  rename(without_strat_dev = '0'  , with_strat_dev = '1')

combined_data <- left_join(per_ppt_format, pre_reb_devs, by = c("ppt", "block_num")) %>%
  mutate(measured_exp = with_strat_dev - without_strat_dev, inferred_exp = pre_reb_dev - without_strat_dev)


head(combined_data)
```

Plot the different explicit measures

```{r}
#descriptives
desc_temp <- combined_data %>%
  group_by(exp, block_num) %>%
  summarize(measured_exp_mean = mean(measured_exp), 
            measured_exp_sd = sd(measured_exp), 
            measured_exp_ci = vector_confint(measured_exp),
            inferred_exp_mean = mean(inferred_exp), 
            inferred_exp_sd = sd(inferred_exp), 
            inferred_exp_ci = vector_confint(inferred_exp),
            n = n())
head(desc_temp)
```

```{r}
p <- desc_temp %>%
  ggplot(aes(block_num, measured_exp_mean, colour = exp)) +
  ggtitle("Measured vs inferred explicit (circles = measured)") +
  geom_point(size = 5, alpha = 0.6, 
             position = position_dodge(width = .6)) + 
  geom_linerange(aes(ymin = measured_exp_mean - measured_exp_ci, ymax = measured_exp_mean + measured_exp_ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6)) +
  geom_point(aes(y = inferred_exp_mean),
             size = 5, alpha = 0.6, shape = 18,
             position = position_dodge(width = .6)) + 
  geom_linerange(aes(ymin = inferred_exp_mean - inferred_exp_ci, ymax = inferred_exp_mean + inferred_exp_ci), 
                 lwd = 5, alpha = 0.4, 
                 position = position_dodge(width = .6)) + 
  scale_y_continuous(name = "explicit learning (°)") +
  scale_x_discrete(name = "block")
  
p
```
Measured explicit learning = markedly lower

